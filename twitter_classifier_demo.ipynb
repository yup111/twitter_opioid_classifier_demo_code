{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from sklearn.metrics import classification_report\n",
    "import keras\n",
    "from keras.models import load_model\n",
    "import xgboost, numpy, string, pandas\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers\n",
    "# import preproce_text \n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import RandomOverSampler \n",
    "import numpy as np\n",
    "import pickle\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feature selection class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureSelect:\n",
    "    def __init__(self, train_x, test_x, trainDF):\n",
    "        self.train_x = train_x\n",
    "        self.test_x = test_x\n",
    "        self.trainDF = trainDF\n",
    "    \n",
    "    def numpy_fillna(self, data):\n",
    "        # Get lengths of each row of data\n",
    "        data = data.toarray()\n",
    "#         for i in data:\n",
    "#             print(i.shape)\n",
    "        lens = numpy.array([len(i) for i in data])\n",
    "#         print(lens)\n",
    "\n",
    "        # Mask of valid places in each row\n",
    "        mask = numpy.arange(lens.max()) < lens[:,None]\n",
    "\n",
    "        # Setup output array and put elements from data into masked positions\n",
    "        out = numpy.zeros(mask.shape, dtype=data.dtype)\n",
    "        out[mask] = numpy.concatenate(data)\n",
    "        return out\n",
    "    \n",
    "    def count_vectors(self):\n",
    "        # create a count vectorizer object\n",
    "        count_vect = CountVectorizer(analyzer=\"word\", token_pattern=r'\\w{1,}') \n",
    "        count_vect.fit(self.trainDF)\n",
    "        \n",
    "        # transform the training and validation data using count vectorizer object\n",
    "        xtrain_count = count_vect.transform(self.train_x)\n",
    "        xtest_count = count_vect.transform(self.test_x)\n",
    "        \n",
    "        return xtrain_count, xtest_count\n",
    "    \n",
    "    def tf_idf(self, select):\n",
    "        if (select==\"word\"):\n",
    "            tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=500)\n",
    "            tfidf_vect.fit(self.trainDF)\n",
    "            xtrain_tfidf =  tfidf_vect.transform(self.train_x)\n",
    "            xtest_tfidf =  tfidf_vect.transform(self.test_x)\n",
    "            \n",
    "#             xtrain_tfidf = self.numpy_fillna(xtrain_tfidf)\n",
    "#             xtest_tfidf = self.numpy_fillna(xtest_tfidf)\n",
    "            \n",
    "            return xtrain_tfidf, xtest_tfidf\n",
    "        elif (select==\"ngram\"):\n",
    "            tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=500)\n",
    "            tfidf_vect_ngram.fit(self.trainDF)\n",
    "            xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(self.train_x)\n",
    "#             xtest_tfidf_ngram =  tfidf_vect_ngram.transform(self.test_x)\n",
    "\n",
    "            return xtrain_tfidf_ngram           \n",
    "        elif (select==\"characters\"):\n",
    "            tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=500)\n",
    "            tfidf_vect_ngram_chars.fit(self.trainDF)\n",
    "            xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(self.train_x) \n",
    "            xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(self.test_x)\n",
    "            \n",
    "            return xtrain_tfidf_ngram_chars, xvalid_tfidf_ngram_chars \n",
    "        \n",
    "    def word_embedding(self):\n",
    "        # load the pre-trained word-embedding vectors \n",
    "        embeddings_index = {}\n",
    "        for i, line in enumerate(open('./glove.twitter.27B.100d.txt')):\n",
    "            values = line.split()\n",
    "            embeddings_index[values[0]] = numpy.asarray(values[1:], dtype='float32')\n",
    "\n",
    "        # create a tokenizer \n",
    "        token = text.Tokenizer()\n",
    "        token.fit_on_texts(self.trainDF)\n",
    "        word_index = token.word_index\n",
    "\n",
    "        # convert text to sequence of tokens and pad them to ensure equal length vectors \n",
    "        train_seq_x = sequence.pad_sequences(token.texts_to_sequences(self.train_x), maxlen=500)\n",
    "        valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(self.test_x), maxlen=500)\n",
    "\n",
    "        # create token-embedding mapping\n",
    "        embedding_matrix = numpy.zeros((len(word_index) + 1, 100))\n",
    "        for word, i in word_index.items():\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "        return train_seq_x, valid_seq_x, embedding_matrix, word_index\n",
    "    def nlp_based(self):\n",
    "        self.trainDF['char_count'] = self.trainDF['text'].apply(len)\n",
    "        self.trainDF['word_count'] = self.trainDF['text'].apply(lambda x: len(x.split()))\n",
    "        self.trainDF['word_density'] = self.trainDF['char_count'] / (trainDF['word_count']+1)\n",
    "        self.trainDF['punctuation_count'] = self.trainDF['text'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation))) \n",
    "        self.trainDF['title_word_count'] = self.trainDF['text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.istitle()]))\n",
    "        self.trainDF['upper_case_word_count'] = self.trainDF['text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()]))\n",
    "        pos_family = {\n",
    "            'noun' : ['NN','NNS','NNP','NNPS'],\n",
    "            'pron' : ['PRP','PRP$','WP','WP$'],\n",
    "            'verb' : ['VB','VBD','VBG','VBN','VBP','VBZ'],\n",
    "            'adj' :  ['JJ','JJR','JJS'],\n",
    "            'adv' : ['RB','RBR','RBS','WRB']\n",
    "        }\n",
    "\n",
    "        # function to check and get the part of speech tag count of a words in a given sentence\n",
    "        def check_pos_tag(x, flag):\n",
    "            cnt = 0\n",
    "            try:\n",
    "                wiki = textblob.TextBlob(x)\n",
    "                for tup in wiki.tags:\n",
    "                    ppo = list(tup)[1]\n",
    "                    if ppo in pos_family[flag]:\n",
    "                        cnt += 1\n",
    "            except:\n",
    "                pass\n",
    "            return cnt\n",
    "\n",
    "        self.trainDF['noun_count'] = self.trainDF['text'].apply(lambda x: check_pos_tag(x, 'noun'))\n",
    "        self.trainDF['verb_count'] = self.trainDF['text'].apply(lambda x: check_pos_tag(x, 'verb'))\n",
    "        self.trainDF['adj_count'] = self.trainDF['text'].apply(lambda x: check_pos_tag(x, 'adj'))\n",
    "        self.trainDF['adv_count'] = self.trainDF['text'].apply(lambda x: check_pos_tag(x, 'adv'))\n",
    "        self.trainDF['pron_count'] = self.trainDF['text'].apply(lambda x: check_pos_tag(x, 'pron'))\n",
    "        \n",
    "    def lda(self):\n",
    "        # train a LDA Model\n",
    "        lda_model = decomposition.LatentDirichletAllocation(n_components=20, learning_method='online', max_iter=20)\n",
    "        X_topics = lda_model.fit_transform(xtrain_count)\n",
    "        topic_word = lda_model.components_ \n",
    "        vocab = count_vect.get_feature_names()\n",
    "\n",
    "        # view the topic models\n",
    "        n_top_words = 10\n",
    "        topic_summaries = []\n",
    "        for i, topic_dist in enumerate(topic_word):\n",
    "            topic_words = numpy.array(vocab)[numpy.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "            topic_summaries.append(' '.join(topic_words))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## machine learning & deeplearning class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelSelect:\n",
    "    def __init__(self, xtrain_x, xtest_x, ytrain_y, ytest_y, vector_name=None):\n",
    "        self.xtrain_x = xtrain_x\n",
    "        self.xtest_x = xtest_x\n",
    "        self.ytrain_y = ytrain_y\n",
    "        self.ytest_y = ytest_y\n",
    "        self.vector_name = vector_name\n",
    "\n",
    "    def train_model(self, classifier, is_neural_net=False, GB=False, epochs=None):\n",
    "        # fit the training dataset on the classifier\n",
    "        if GB:\n",
    "            classifier.fit(self.xtrain_x.tocsc(), self.ytrain_y)\n",
    "        elif is_neural_net:\n",
    "            transfered_train_y = pandas.get_dummies(self.ytrain_y).values\n",
    "            transfered_test_y = pandas.get_dummies(self.ytest_y).values\n",
    "            classifier.fit(self.xtrain_x, \n",
    "                           transfered_train_y, \n",
    "                           epochs=epochs, \n",
    "                           batch_size=32,\n",
    "                          callbacks=[plot_losses],\n",
    "                          validation_data=(self.xtest_x, transfered_test_y))\n",
    "            predictions = classifier.predict(self.xtest_x)\n",
    "            class_labels = numpy.argmax(predictions, axis=1)\n",
    "    #             acc = metrics.accuracy_score(class_labels, self.ytest_y)\n",
    "    #             precision_recall_fscore_support(self.ytest_y, class_labels, average='macro')\n",
    "    #             transfered_test = pandas.get_dummies(self.ytest_y).values\n",
    "    #             classifier.evaluate(self.xtest_x, transfered_test, verbose=0)\n",
    "            return precision_recall_fscore_support(self.ytest_y, class_labels, average='weighted'), classifier\n",
    "\n",
    "        else:\n",
    "            classifier.fit(self.xtrain_x, self.ytrain_y)\n",
    "\n",
    "        # predict the labels on validation dataset\n",
    "        if GB:\n",
    "            predictions = classifier.predict(self.xtest_x.tocsc())\n",
    "        else:\n",
    "            predictions = classifier.predict(self.xtest_x)\n",
    "\n",
    "        return precision_recall_fscore_support(self.ytest_y, predictions, average='weighted'), classifier\n",
    "\n",
    "    def nb(self):\n",
    "        accuracy, classifier = self.train_model(naive_bayes.MultinomialNB())\n",
    "        return accuracy, classifier\n",
    "\n",
    "    def linear_reg(self):\n",
    "        accuracy, classifier = self.train_model(linear_model.LogisticRegression())\n",
    "        return accuracy, classifier\n",
    "\n",
    "    def svm(self):\n",
    "        accuracy, classifier = self.train_model(svm.SVC())\n",
    "        return accuracy, classifier\n",
    "\n",
    "    def random_forest(self):\n",
    "        accuracy, classifier = self.train_model(ensemble.RandomForestClassifier())\n",
    "        return accuracy, classifier\n",
    "\n",
    "    def gradient_boost(self):\n",
    "        accuracy, classifier = self.train_model(xgboost.XGBClassifier(), GB=True)\n",
    "        return accuracy, classifier\n",
    "\n",
    "    def create_model_architecture(self, epochs=None):\n",
    "        # create input layer \n",
    "        input_layer = layers.Input((numpy.size(self.xtrain_x, 1), ), sparse=True)\n",
    "\n",
    "        # create hidden layer\n",
    "        hidden_layer = layers.Dense(100, activation=\"relu\")(input_layer)\n",
    "\n",
    "        # create output layer\n",
    "        output_layer = layers.Dense(4, activation=\"softmax\")(hidden_layer)\n",
    "\n",
    "        classifier = models.Model(inputs = input_layer, outputs = output_layer)\n",
    "        classifier.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        accuracy = self.train_model(classifier, is_neural_net=True, epochs=epochs)\n",
    "\n",
    "        return accuracy, classifier\n",
    "\n",
    "    def create_cnn(self, epochs=None, embedding_matrix=None, word_index=None, optimizer='rmsprop'):\n",
    "        # Add an Input Layer\n",
    "        input_layer = layers.Input((numpy.size(self.xtrain_x, 1), ))\n",
    "\n",
    "        # Add the word embedding Layer\n",
    "        embedding_layer = layers.Embedding(len(word_index) + 1, 100, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "        embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "        # Add the convolutional Layer\n",
    "        conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n",
    "\n",
    "        # Add the pooling Layer\n",
    "        pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
    "\n",
    "        # Add the output Layers\n",
    "        output_layer1 = layers.Dense(64, activation=\"relu\")(pooling_layer)\n",
    "        output_layer1 = layers.Dropout(0.2)(output_layer1)\n",
    "        output_layer2 = layers.Dense(2, activation=\"softmax\")(output_layer1)\n",
    "\n",
    "        # Compile the model\n",
    "        model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "        model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        accuracy = self.train_model(model, is_neural_net=True, epochs=epochs)\n",
    "\n",
    "\n",
    "        return accuracy, model\n",
    "\n",
    "    def create_rnn_lstm(self, epochs=None, embedding_matrix=None, word_index=None, optimizer=None):\n",
    "        # Add an Input Layer\n",
    "        input_layer = layers.Input((numpy.size(self.xtrain_x, 1), ))\n",
    "\n",
    "        # Add the word embedding Layer\n",
    "        embedding_layer = layers.Embedding(len(word_index) + 1, 100, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "        embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "        # Add the LSTM Layer\n",
    "        lstm_layer = layers.LSTM(128)(embedding_layer)\n",
    "\n",
    "        # Add the output Layers\n",
    "        output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
    "        output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "        output_layer2 = layers.Dense(3, activation=\"softmax\")(output_layer1)\n",
    "\n",
    "        # Compile the model\n",
    "        model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "        model.compile(optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        accuracy = self.train_model(model, is_neural_net=True, epochs=epochs)\n",
    "\n",
    "        return accuracy, model\n",
    "\n",
    "    def create_rnn_gru(self, epochs=None, embedding_matrix=None, word_index=None, optimizer=None):\n",
    "        # Add an Input Layer\n",
    "        input_layer = layers.Input((numpy.size(self.xtrain_x, 1), ))\n",
    "\n",
    "        # Add the word embedding Layer\n",
    "        embedding_layer = layers.Embedding(len(word_index) + 1, 100, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "        embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "        # Add the GRU Layer\n",
    "        lstm_layer = layers.GRU(100)(embedding_layer)\n",
    "\n",
    "        # Add the output Layers\n",
    "        output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
    "        output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "        output_layer2 = layers.Dense(4, activation=\"softmax\")(output_layer1)\n",
    "\n",
    "        # Compile the model\n",
    "        model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "        model.compile(optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        accuracy = self.train_model(model, is_neural_net=True, epochs=epochs)\n",
    "\n",
    "        return accuracy, model\n",
    "\n",
    "    def create_bidirectional_rnn(self, epochs=None, embedding_matrix=None, word_index=None, optimizer=None):\n",
    "        # Add an Input Layer\n",
    "        input_layer = layers.Input((numpy.size(self.xtrain_x, 1), ))\n",
    "\n",
    "        # Add the word embedding Layer\n",
    "        embedding_layer = layers.Embedding(len(word_index) + 1, 100, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "        embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "        # Add the LSTM Layer\n",
    "        lstm_layer = layers.Bidirectional(layers.GRU(128))(embedding_layer)\n",
    "\n",
    "        # Add the output Layers\n",
    "        output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
    "        output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "        output_layer2 = layers.Dense(4, activation=\"softmax\")(output_layer1)\n",
    "\n",
    "        # Compile the model\n",
    "        model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "        model.compile(optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        accuracy = self.train_model(model, is_neural_net=True, epochs=epochs)\n",
    "\n",
    "        return accuracy, model\n",
    "\n",
    "    def create_rcnn(self, epochs=None, embedding_matrix=None, word_index=None, optimizer=None):\n",
    "        # Add an Input Layer\n",
    "        input_layer = layers.Input((numpy.size(self.xtrain_x, 1), ))\n",
    "\n",
    "        # Add the word embedding Layer\n",
    "        embedding_layer = layers.Embedding(len(word_index) + 1, 100, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "        embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "        # Add the recurrent layer\n",
    "        rnn_layer = layers.Bidirectional(layers.GRU(50, return_sequences=True))(embedding_layer)\n",
    "\n",
    "        # Add the convolutional Layer\n",
    "        conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n",
    "\n",
    "        # Add the pooling Layer\n",
    "        pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
    "\n",
    "        # Add the output Layers\n",
    "        output_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\n",
    "        output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "        output_layer2 = layers.Dense(4, activation=\"softmax\")(output_layer1)\n",
    "\n",
    "        # Compile the model\n",
    "        model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "        model.compile(optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        accuracy = self.train_model(model, is_neural_net=True, epochs=epochs)\n",
    "\n",
    "        return accuracy, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utilization tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlotLosses(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.i = 0\n",
    "        self.x = []\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        \n",
    "        self.fig = plt.figure()\n",
    "        \n",
    "        self.logs = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \n",
    "        self.logs.append(logs)\n",
    "        self.x.append(self.i)\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        self.i += 1\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        plt.plot(self.x, self.losses, label=\"loss\")\n",
    "        plt.plot(self.x, self.val_losses, label=\"val_loss\")\n",
    "        plt.legend()\n",
    "        plt.show();\n",
    "        \n",
    "def preprocess_text(row):\n",
    "    return preproce_text.sanitize(str(row['clean_text']))\n",
    "\n",
    "def trans_row(row):\n",
    "    return str(row['text'])\n",
    "\n",
    "def train_test_split_(inputfile):\n",
    "    # read train-test data from a file\n",
    "    train_test_data = pd.read_csv(inputfile, target_size, output)\n",
    "    grouped = train_test_data.groupby('label')\n",
    "    \n",
    "    train_data = pd.DataFrame()\n",
    "    test_data = pd.DataFrame()\n",
    "\n",
    "    sample_size = target_size/len(train_test_data)\n",
    "\n",
    "    for key, group in grouped:\n",
    "        train_sample = group.sample(int(len(group)*sample_size))\n",
    "        train_data = train_data.append(train_sample)\n",
    "        \n",
    "    if len(train_data) < target_size:\n",
    "        train_data = train_data.append(train_data.sample(target_size-len(train_data)))\n",
    "        \n",
    "    train_data.to_csv(output, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## classifier Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pd.read_excel(\"./palliative_care_annotation.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = corpus[\"text\"].tolist()\n",
    "labels = corpus[\"label\"].tolist()\n",
    "\n",
    "plot_losses = PlotLosses()\n",
    "\n",
    "\n",
    "trainDF = pandas.DataFrame()\n",
    "trainDF['text'] = texts\n",
    "trainDF['label'] = labels\n",
    "trainDF['text_str'] = trainDF.apply(trans_row, axis=1)\n",
    "\n",
    "featureSelect = FeatureSelect(trainDF['text_str'], trainDF['text_str'], trainDF[\"text_str\"])\n",
    "xtrain_word = featureSelect.tf_idf(select=\"ngram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_word_old, xtest_word, train_y, test_y = model_selection.train_test_split(xtrain_word, trainDF_label, stratify=trainDF_label, test_size=0.2)    \n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "test_y = encoder.fit_transform(test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelSelect_word = ModelSelect(xtrain_word_new, xtest_word, ml_train_y, test_y)\n",
    "model_accuracy, model = modelSelect_word.linear_reg()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
